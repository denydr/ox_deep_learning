{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14aaa39a-8327-4451-bc93-27f325ce8d01",
   "metadata": {},
   "source": [
    "# Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b574b26",
   "metadata": {},
   "source": [
    "Speech recognition is the ability of computer software to identify words and phrases in spoken language and convert them to human-readable text. In this tutorial, you will learn how you can convert speech to text in Python using the SpeechRecognition library.\n",
    "\n",
    "As a result, we do not need to build any machine learning model from scratch, this library provides us with convenient wrappers for various well-known public speech recognition APIs (such as Google Cloud Speech API, IBM Speech To Text, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbea647-0f69-4f96-aadd-d770b0e4044e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 | Install New Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad34372-0fe2-4c01-9509-981441bbf3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from SpeechRecognition) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from requests>=2.26.0->SpeechRecognition) (2024.7.4)\n",
      "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m222.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.10.4 pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Speech Recognition. Might have to install from terminal\n",
    "%pip install SpeechRecognition pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1caeea-2cb6-4299-94b8-fbbb8900e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/dd/PycharmProjects/ox_deep_learning/.venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m426.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 regex-2024.7.24 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Sklearn sentiment analysis\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56b55f4-b61d-4d70-bd68-a430687ec011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=cf439cdd748ba4ff9d3154f9c6c61f9f9da5bf552f4c1309918254f683652f0e\n",
      "  Stored in directory: /Users/dd/Library/Caches/pip/wheels/56/30/c5/576bdd729f3bc062d62a551be7fefd6ed2f761901568171e4e\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Converts video & audio footage between operating systems\n",
    "%pip install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51a72a-78b2-482b-92aa-92a466e80843",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 | Transcribing Small Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1991b5a-ad91-4da3-96ad-584820e829b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Surpress project warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aafb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alright, let's get started, installing the library using pip:\n",
    "import speech_recognition as sr\n",
    "\n",
    "#We gonna use Google Speech Recognition here, as it's straightforward \n",
    "#and doesn't require any API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30bbf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading from a File\n",
    "#Make sure you have an audio file in the current directory that \n",
    "#contains English speech\n",
    "\n",
    "filename = \"machine-learning_speech-recognition_16-122828-0002.wav\"\n",
    "\n",
    "#This file was grabbed from the LibriSpeech dataset, but you can use \n",
    "#any audio WAV file you want, just change the name of the file, \n",
    "#let's initialize our speech recognizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0eb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the recognizer\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb6d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe you're just talking nonsense\n"
     ]
    }
   ],
   "source": [
    "#The below code is responsible for loading the audio file, and \n",
    "#converting the speech into text using Google Speech Recognition:\n",
    "\n",
    "# open the file\n",
    "with sr.AudioFile(filename) as source: #calls the file\n",
    "    # listen for the data (load audio to memory)\n",
    "    audio_data = r.record(source) #understand it is recording\n",
    "    # recognize (convert from speech to text)\n",
    "    text = r.recognize_google(audio_data) # convert speech-to-text\n",
    "    print(text)\n",
    "    \n",
    "#here is my result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a9a28",
   "metadata": {},
   "source": [
    "The above code works well for small or medium size audio files. In the next section, we gonna write code for large files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc4f32-3368-4dd1-a1ab-91af36567a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3 | Transcribing Large Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef823314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Large Audio Files\n",
    "\n",
    "#If you want to perform speech recognition of a long audio file, \n",
    "#then the below function handles that quite well:\n",
    "# importing libraries \n",
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5f0a7d-ac1e-4ac5-8e1e-5c784094767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431945e",
   "metadata": {},
   "source": [
    "The above function uses split_on_silence() function from pydub.silence module to split audio data into chunks on silence. The min_silence_len parameter is the minimum length of silence to be used for a split.\n",
    "\n",
    "silence_thresh is the threshold in which anything quieter than this will be considered silence, I have set it to the average dBFS minus 14, keep_silence argument is the amount of silence to leave at the beginning and the end of each chunk detected in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25461a",
   "metadata": {},
   "source": [
    "These parameters won't be perfect for all sound files, try to experiment with these parameters with your large audio needs.\n",
    "\n",
    "After that, we iterate over all chunks and convert each speech audio into text, and then adding them up altogether, here is an example run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099cfb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks/chunk1.wav : What do you call a factory that makes ok products. \n",
      "audio-chunks/chunk2.wav : A satisfactory. \n",
      "audio-chunks/chunk3.wav : Dear math grow up in solve your own problems. \n",
      "audio-chunks/chunk4.wav : What did the janitor say when he jumped out of the clos. \n",
      "audio-chunks/chunk5.wav : Surprise. \n",
      "audio-chunks/chunk6.wav : What did the ocean say to the beach. \n",
      "audio-chunks/chunk7.wav : Nothing you just wave. \n",
      "audio-chunks/chunk8.wav : I heard that the national dutch crime agency is better than the national crime agency of the uk. \n",
      "audio-chunks/chunk9.wav : Oh sorry well that's supposed to be funny. \n",
      "\n",
      "Full text: What do you call a factory that makes ok products. A satisfactory. Dear math grow up in solve your own problems. What did the janitor say when he jumped out of the clos. Surprise. What did the ocean say to the beach. Nothing you just wave. I heard that the national dutch crime agency is better than the national crime agency of the uk. Oh sorry well that's supposed to be funny. \n"
     ]
    }
   ],
   "source": [
    "path= \"barakgotjokes.wav\"\n",
    "print(\"\\nFull text:\", get_large_audio_transcription(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ee01d-b2d0-4234-b72b-1fb353d7a68f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 | Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a7b9c4",
   "metadata": {},
   "source": [
    "Sentiment analysis is the use of natural language to classify the opinion of people. It helps to classify words (written or spoken) into positive, negative, or neutral depending on the use case. The sentiment analyzed can help identify the pattern of a product; it helps to know what the users are saying and take the necessary steps to mitigate any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0a5cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import pydub\n",
    "import speech_recognition as sr\n",
    "\n",
    "#install ffmpeg to work with different video format (Linux)\n",
    "#sudo snap install ffmpeg\n",
    "import ffmpeg\n",
    "\n",
    "#install nltk-- Natural language tool kit from sklearn\n",
    "#pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4e46923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atoth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\atoth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download some packages from nltk; pretrained sentiment models\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f4ebef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let’s write a function that reads our audio data and converts the data format to a .wav file from any other audio format (mp3, mp4, etc.).\n",
    "#You won't need it for this exercise, but you'll have it for future references.\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "\n",
    "def convert_to_wav(filename):\n",
    "\n",
    "  \"\"\"Takes an audio file of non .wav format and converts to .wav\"\"\"\n",
    "  # Import audio file\n",
    "  audio = AudioSegment.from_file(filename)\n",
    "\n",
    "  # Create new filename\n",
    "  new_filename = filename.split(\".\")[0] + \".wav\"\n",
    "\n",
    "  # Export file as .wav\n",
    "  audio.export(new_filename, format='wav')\n",
    "  print(f\"Converting {filename} to {new_filename}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137ecf8",
   "metadata": {},
   "source": [
    "In the block of code above, the AudioSegment class of the pydub library was instantiated (it contains many of the methods you would be using) and the SpeechRecognition library was imported as sr. Also, the function takes in the argument filename (name of the audio file) and uses the from_file method of the AudioSegment class to read the filename and save it as an audio variable. The next line uses the method split to separate the filename from its extension and add it to the .wav using the ‘+’ arithmetic which will concatenate the string. The result is then saved as variable new_filename. Lastly, the audio file was further exported in the .wav file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94e87a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(filename):\n",
    "    \"\"\"Takes a .wav format audio file and transcribes it to text.\"\"\"\n",
    "    # Setup a recognizer instance\n",
    "    recognizer = sr.Recognizer()\n",
    "    \n",
    "    #Import the audio file and convert to audio data\n",
    "    audio_file = sr.AudioFile(filename)\n",
    "    with audio_file as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        \n",
    "        # Return the transcribed text\n",
    "        return recognizer.recognize_google(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113259c9-1f13-4d41-9173-fa8cda963026",
   "metadata": {},
   "source": [
    "Finally, let's transcribe an audio file and analyse its sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3f727d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you call a factory that makes okay products a satisfactory dear math grow up in solve your own problems what did the janitor say when he jumped out of the closet supplies what did the ocean say to the beach nothing you just waved I heard that the national Dutch crime agency is better than the National Crime agency of the UK oh sorry well that's supposed to be funny\n",
      "{'neg': 0.128, 'neu': 0.677, 'pos': 0.195, 'compound': 0.5719}\n"
     ]
    }
   ],
   "source": [
    "#from speech_helpers import convert_to_wav, show_pydub_stats, transcribe_audio\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "filename = \"barakgotjokes.wav\"\n",
    "\n",
    "trans_text = transcribe_audio(filename)\n",
    "print(trans_text)\n",
    "print(sid.polarity_scores(trans_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90145a52",
   "metadata": {},
   "source": [
    "In the code above, I imported the functions we wrote earlier and the SentimentIntensityAnalyzer from the Vader model of NLTK. An instance of the analyzer is stored in sid and the audio data is saved in the filename (to be used as argument of the function) and I named the .wav version of the data in variable new_name (this will be generated when audio is changed to .wav with convert_to_wav function). I transcribed the audio to test with the function and used the polarity_scores of the SentimentIntensityAnalyzer to get the score of the sentiments.\n",
    "\n",
    "The results show the negative (neg), neutral (neu), positive (pos), and compound. The corresponding values for each key show the degree to which the word is negative, neutral, positive, and a combined inference (compound). From the results, we could see that the world is more neutral. However, the compound is scaled within a -1 and +1 such that as scores move closer to -1, the more negative and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec8d8d-7046-4606-9e90-d4fd37af79ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5 | Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f836465",
   "metadata": {},
   "source": [
    "Exploratory data analysis is the act of analyzing a dataset to show its main attributes or characteristics. For this project, we shall be using pydub; a Python library for manipulation of audio with a simple and easy interface to extract the following from the audio data: Channels, sample width, frame rate, and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea7589f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function below will generate the above-listed attributes of the audio data:\n",
    "\n",
    "def show_pydub_stats(filename):\n",
    "    \"\"\"Returns different audio attributes related to an audio file.\"\"\"\n",
    "    # Create AudioSegment instance\n",
    "    audio_segment = AudioSegment.from_file(filename)\n",
    "    \n",
    "    # Print audio attributes and return AudioSegment instance\n",
    "    print(f\"Channels: {audio_segment.channels}\")\n",
    "    print(f\"Sample width: {audio_segment.sample_width}\")\n",
    "    print(f\"Frame rate (sample rate): {audio_segment.frame_rate}\")\n",
    "    print(f\"Frame width: {audio_segment.frame_width}\")\n",
    "    print(f\"Length (ms): {len(audio_segment)}\")\n",
    "    return audio_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb0373ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels: 2\n",
      "Sample width: 2\n",
      "Frame rate (sample rate): 48000\n",
      "Frame width: 4\n",
      "Length (ms): 27093\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = \"barakgotjokes.wav\"\n",
    "    show_pydub_stats(path)\n",
    "except Exception as E: \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
